{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMXqEdja2nUFxjRnsey8nwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ceciliawangwang/object_detection_tutorial/blob/main/Tutorial_Object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ojbect detection tutorial"
      ],
      "metadata": {
        "id": "tNqG4H1WVt7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Introduction\n",
        "In this tutorial, we will use You Only Look Once (YOLO) to train an object detection model in Python.\n",
        "\n",
        "The objectives of this tutorial are:\n",
        "\n",
        "*   Prepare object detection annotation data\n",
        "*   Understand the fundamentals of YOLO model\n",
        "*   Train an object detection model\n"
      ],
      "metadata": {
        "id": "XozgpYZ9V12J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Configure environment\n",
        "\n",
        "**2.1 Get start with Google Colab**\n",
        "\n",
        "In this tutorial, we will access to a pre-configured Jupyter Notebooks running on *Google Colaboratory* : https://colab.research.google.com/\n",
        "\n",
        "It is a free, cloud-based development environment provided by Google. It allows users to write and run Python code in a web browser, with access to powerful computing resources such as GPUs and TPUs.\n",
        "\n"
      ],
      "metadata": {
        "id": "t0TJoGcxPLkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Mount the Google Drive**\n"
      ],
      "metadata": {
        "id": "T-6kDJWnOn5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skHQp530Oy3_",
        "outputId": "e7f82188-bb3d-43af-960f-bc185c035418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Import libraries**\n",
        "\n",
        "Install all the libraries used in YOLO by installing `ulbralytics`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e6N-YsIv1Zpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dts-PYpvSh1",
        "outputId": "71c82fcd-a30a-4eff-8be6-85dc46d6a536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.173-py3-none-any.whl (614 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/614.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/614.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m604.2/614.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.2/614.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (16.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: ultralytics\n",
            "Successfully installed ultralytics-8.0.173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# import torch\n",
        "\n",
        "from ultralytics import YOLO\n"
      ],
      "metadata": {
        "id": "z_Q5vCBEPOyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this tutorial we will use GPU to speed up the trainning process of the image segmentaion. Here is the settings in Google Colaboratory:\n",
        "`Runtime >> Change runtime type >> Hardware accelerator >> GPU`\n",
        "\n",
        "Test the connect of gpu resources, please type in the `torch.cuda.is_available()`"
      ],
      "metadata": {
        "id": "KJz3QGMdawKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Z8waMg32tOXg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "9e29f236-3ac2-4846-9fd9-1bb9da89d531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1109c9096124>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDEVICE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Data Preparation and Annotation\n",
        "\n",
        "### 3.1 Data annotation:\n",
        "\n",
        "Here are some tools that we could use to annotate data.\n",
        "\n",
        "*   cvat\n",
        "*   LabelMe\n",
        "\n",
        "### 3.2 Dataset\n",
        "In this tutorial, we will use the data from: https://www.kaggle.com/datasets/sakshamjn/vehicle-detection-8-classes-object-detection"
      ],
      "metadata": {
        "id": "BzYXLYEs0mVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 YOLO\n",
        "\n",
        "You Only Look Once (YOLO) is a popular and real-time object proposed by Joseph Redmon in 2015. Using a single convolutional neural network (CNN). It has a lot of advantages, such as fast speed, high detection accuracy, open sourced, and better generalization.\n",
        "It’s done by dividing an image into a grid and predicting bounding boxes and class probabilities for each cell in a grid.\n",
        "\n",
        "\n",
        "### 4.1 YOLO Data Format\n",
        "\n",
        "*   *.jpg\n",
        "*   *.txt\n",
        "\n",
        "We should add a custom dataset file `data.yaml`\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "train: path1\n",
        "val: path2\n",
        "\n",
        "nc: 2   #number of classes\n",
        "names: ['class1', 'class2']\n",
        "```"
      ],
      "metadata": {
        "id": "ZZupr6IL702_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Folder structure\n",
        "\n",
        "\n",
        "Set up the folder structure as follows:\n",
        "\n",
        "```\n",
        "root/\n",
        "    images/\n",
        "        train/\n",
        "            image1.jpg\n",
        "            image2.jpg\n",
        "            ...\n",
        "        val/\n",
        "            image1.jpg\n",
        "            image2.jpg\n",
        "            ...\n",
        "\n",
        "    labels/\n",
        "        train/\n",
        "            label1.txt\n",
        "            label2.txt\n",
        "            ...\n",
        "        val/\n",
        "            image1.jpg\n",
        "            image2.jpg\n",
        "            ...\n",
        "```\n"
      ],
      "metadata": {
        "id": "D0Y4CDKkQ0A9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 YOLO in Python\n",
        "### 5.1 load a model\n",
        "Models could be found in [YOLO models](https://github.com/ultralytics/ultralytics).\n",
        "\n",
        "\n",
        "*   YOLOv8n (Nano model)\n",
        "*   YOLOv8s (Small model)\n",
        "*   YOLOv8m (Medium model)\n",
        "*   YOLOv8l (Large model)\n",
        "*   YOLOv8x (Extra large model)\n",
        "\n",
        "Here we chose the smallest and the most light-weight one YOLOv8n to showcase the training process."
      ],
      "metadata": {
        "id": "BXWYuR3XEcmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/od_dataset'"
      ],
      "metadata": {
        "id": "8X5svHICbSlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a model\n",
        "model = YOLO('yolov8n.yaml') # build a new model from scratch, nano model\n",
        "# model = YOLO('yolov8n.pt') #load a pretrained model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J--T8SXVtLh1",
        "outputId": "b0979d75-f171-4405-b876-f2908055468a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
            "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Train the model"
      ],
      "metadata": {
        "id": "wsKSw0guFQvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "results = model.train(\n",
        "    data ='/content/drive/MyDrive/od_dataset/data.yaml',\n",
        "    #imgsz = ,\n",
        "    epochs= 3\n",
        "    #batch =\n",
        "    )"
      ],
      "metadata": {
        "id": "4NLLPc9aEbAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cecf3ad0-1a48-4da9-e967-17d74f4ee066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.173 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/content/drive/MyDrive/od_dataset/data.yaml, epochs=3, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752872  ultralytics.nn.modules.head.Detect           [8, [64, 128, 256]]           \n",
            "YOLOv8n summary: 225 layers, 3012408 parameters, 3012392 gradients\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/od_dataset/labels/train.cache... 73 images, 0 backgrounds, 0 corrupt: 100%|██████████| 73/73 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/od_dataset/labels/val.cache... 13 images, 0 backgrounds, 0 corrupt: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/detect/train2/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        1/3       2.7G      4.231      6.083      4.288         18        640: 100%|██████████| 5/5 [00:03<00:00,  1.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
            "                   all         13         41          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        2/3      2.54G      4.235       6.07      4.274         16        640: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\n",
            "                   all         13         41          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "        3/3      2.56G      4.251      6.089      4.303         20        640: 100%|██████████| 5/5 [00:00<00:00,  5.05it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.35it/s]\n",
            "                   all         13         41          0          0          0          0\n",
            "\n",
            "3 epochs completed in 0.003 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.173 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
            "                   all         13         41          0          0          0          0\n",
            "Speed: 0.2ms preprocess, 2.0ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = model.val() # evaluate model performance on the validation set"
      ],
      "metadata": {
        "id": "nbC2BC8C21as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3 evaluate the performance\n",
        "Evalue the model performance on the validation dataset."
      ],
      "metadata": {
        "id": "vUOy9x5YFTlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.val()"
      ],
      "metadata": {
        "id": "nGToYFfhGS4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba4d6d4-65e6-43b7-8d93-0a265d164aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.173 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLOv8n summary (fused): 168 layers, 3007208 parameters, 0 gradients\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/od_dataset/labels/val.cache... 13 images, 0 backgrounds, 0 corrupt: 100%|██████████| 13/13 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
            "                   all         13         41          0          0          0          0\n",
            "Speed: 0.2ms preprocess, 11.5ms inference, 0.0ms loss, 3.1ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/val\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Predict\n",
        "You could predict your own image by using the following code."
      ],
      "metadata": {
        "id": "Y9krh4OwFYaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_train = YOLO('/content/runs/detect/train/weights/best.pt')\n",
        "model('image_path')"
      ],
      "metadata": {
        "id": "YhhGjYUAGiyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOLO also supports classification, segmentation and pose detection tasks in images and videos. Please check the offical tutorial for more details."
      ],
      "metadata": {
        "id": "NPVWk74uGygp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "In this tutorial, we had a detailed walkthrough to train the YOLOv8 models on a custom dataset. In the process, we also carried out a small real-world training experiment for pothole detection.\n",
        "\n",
        "The experiments revealed that training object detection models on small objects could be challenging even with sufficient samples. We could observe this as training for 50 epochs was insufficient, and the mAP graphs were still increasing. Also, with smaller objects, larger object detection models (YOLOv8 Medium vs Nano in this case) seem to perform better when carrying out detection on new images and videos."
      ],
      "metadata": {
        "id": "ze0kJsHWQ_XF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"
      ],
      "metadata": {
        "id": "qsjO6WfXxmgI"
      }
    }
  ]
}